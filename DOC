>> NETWORK RAID FILESYSTEM - FreeUni / OS / Final Project

Vache Katsadze - vkats15@freeuni.edu.ge



---------------------------------------------------------
---------------------------------------------------------
CONFIG FILE PARSER
---------------------------------------------------------
---------------------------------------------------------


>> Data Structures 

struct Disk_Config
{
	char* diskname; 
	char* mountpoint;
	int raid;
	char** servers;  //სერვერების მასივი (მაგ. "127.0.0.1:10001", "127.0.0.1:10002", ...) 
	char* hotswap;
	int num_servers;
};

struct Config{
	char* errorlog;   
	char* cache_size;
	char* cache_replacement;
	int timeout;
	struct Disk_Config * disk_configs;  //Disk_Config სტრუქტურების მასივი
	int num_storages;
};

>> Algorithms

დასაპარსად ვიყენებ fscanf - ს, რომელსაც პირდაპი ვეუბნები რა სახით უნდა იყოს ფაილში კონფიგები,
თუ რომელიმე არასწორად არის შეყვანილი fscanf მიბრუნებს რაოდენობას თუ რამდენი წაიკითხა სწორად,
შესაბამისად ეს მიდგომა error handling შიც მეხმარება.
მაგ.  

int res = fscanf(fid, "errorlog = %s cache_size = %s cache_replacement = %s timeout = %d", 
		config_st->errorlog, config_st->cache_size, config_st->cache_replacement, &(config_st->timeout));

სადაც config_st არის ზემოთ აღწერილი სტრუქტურა, რომელსაც parser ის გამოყენებისას net_raid_client
გადმომცემს

Config file - ის დაპარსვა იწყება ზოგადი კონფიგების დაპარსვით და შემდეგ უკვე ფაილის ბოლომდე 
ვპარსავ დისკების კონფიგებს. 



---------------------------------------------------------
---------------------------------------------------------
RAID 1  |  MIRRORING
---------------------------------------------------------
---------------------------------------------------------


>> Data Structures 

ყველა სტრუქტურა აღწერილია utils.h ფაილში, რომელიც net_raid_client მაც და
net_raid_server მაც იცის



//syscall num enum
enum syscall {
	X,          // enum ის პირველი (მე-0) წევრია რაღაც X, რომლის არსებობის მიზეზია, 
				// ის რომ შეცდომის შემთხვევაში 0 არ წაეკთხა სერვერს და არასასურველი
				// syscall ის გამოძახება არ მომხდარიყო
	GETATTR,
	MKNOD,
	... 
};



//write state enum, გამოიყენება stable storage - ის დროს რომ ვუთხრა სერვერს, ახლა უეჭველად 
//წასაკითხად მინდა ფაილი და ხელახლა გამიხსენი "w"/"a" mode ით, თუ პირველი chunk ია
//"w" თი რომ წაშალოს ყველაფერი ხოლო დანარჩენისთვის "a", რომ მიაწებოს ბოლოში chunk ები
enum write_state{
	DEFAULT,    
	BU_FIRST_CHUNK,
	BU_REST,
};



//Metadata ინფო რომელსაც ყოველი syscall ის გამოძახებისას ვაგზავნი სერვერზე, ამ სტრუქტურაში
//არის ყველა syscall ისთვის საჭირო გადასაგზავნი დატა, ხოლო მიღებისას უკვე სერვერმა იცის
//რომელ მათგანს გაუგზანიდა სერვერი და შესაბამისად გამოიყენებს
struct Metadata{
	enum syscall func_num;
	char path[256];
	mode_t mode;
	...
};



//NRF_Data (net raid fs data) არის სტრუქტურა რომელიც გადაეცემა fuse ს ყოველი დისკის გაშვებისას 
//სხვადასხვა შესაბამისი ინფოთი შევსებული 
struct NRF_Data
{
	struct Cache * cache;           //ქეშ სტრუქტურა - ქვემოთ განვიხილავ
	char diskname[256];             //დისკის სახელი - ლოგირებისთვის
	struct Server servers[2];       //რადგან მხოლოდ raid 1 ის იმპლემენტაციაა პირდაპირ ორი სერვერი
	FILE* logfile;                  //ლოგფაილი
	int srtw_idx;                   //მეორე სერვერის მიერ დაბრუნებული ფაილ დესკრიპტორების რაოდენობა
	struct srtw_file ** srtw_files; //მეორე სერვერის მიერ დაბრუნებული ფაილ დესკრიპტორების შემნახავი სტრუქტურა
};



//გამოიყენება მეორე სერვერის fd ების შესანახად open ის დროს, რომ შედგომ write ს სწორი fd გადავცე
struct srtw_file
{
	char file_name[256];
	int fd;	
};


>> Algorithms


main იწყება config file - ის დაპარსვით, რომელიც user მა გადმოსცა პროგრამის გაშვებისას, 
შემდეგ ხდება disk ების რაოდენობაჯერ და fork ვა, სადაც ყოველისთვის ხდება NRF_Data ს, 
კლიენტებისა და cache ის ინიციალიზაცია, ბოლოს კი გამოიძახება fuse_main რომელსაც გადაეცემა
არგუმენტებში config file ის მაგივრად შესაბამისი დისკიდან ამოღებული mountpoint path, ბოლოს კი
ეშვება wait, რომელიც ელოდება ყველა ამ და fork ილ შვილს.


//FUSE

1. getattr
ხდება მხოლოდ პირველი სერვერიდან stat ის წამოღება, კლიენტი უგზავნის სერვერს ფაილის სახელს 
და სერვერი იყენებს lstat syscall ს, რომელიც struct stat ში უწერს შესაბამის ინფორმაციას, რომელიც
შემდგომ კლიენტთან ბრუნდება

2. mknod (create - ს ალტერნატივა, რომელსაც მხოლოდ ფაილის შესაქმნელად ვიყენებ)
კლიენტი პირველ სერვერს უგზავნის ფაილის სახელსა და mode პარამეტრს, სერვერი კი ხსნის ამ სახელით, ფაილს
თუ არ არის ავტომატურად შექმნის და მერე არის შემოწმება, რომ თუ გაიხსნა დახუროს და ხელში შეგვრჩება 
შექმნილი ფაილი, ხოლო კლიენტის მხარეს თუ პირველმა სერვერმა გვითხრა რო შექმნა ფაილი მხოლოდ ამის შემდეგ 
ხდება მეორე სერვერზე შექმნა

3.4.5.6.7. mkdir / unlink / rmdir / rename / truncate 
ყველა შემთხვევაში ხდება ანალოგიური, პირველი სერვერიდან დასტურის შემდგომ მიდის რექვესტი მეორე სერვერზე, 
ხოლო თვითონ სერვერზე გამოიძახება შესაბამისი syscall

8. open
ფაილი იხსნება ორივე სერვერზე, თუმცა fuse ის file_info სტრუქტურას აქვს მხოლოდ ერთი fh სანახი
დესკრიპტორისთვის, ამიტომ ამისთვის ვიყენებ ზემოთ აღწერილ სტრუქტურას srtw_file, რომელთა მასივიც აქვს
NRF_Data - ს, რომელიც fuse ის private data შია, მეორე სერვერიდან დაბრუნებული fd სახელთან ერთად შეინახება ამ 
სტრუქტურების მასივში, open შივე ხდება stable storage ს მთავარი ლოგიკის იმპლემენტაცია თუმცა მაგას შემდგომ აღვწერ

9. read
დატა იკითხება მხოლოდ პირველი სერვერიდან, caching ის ლოგიკის იმპლემენტაციაც აქ არის თუმცა ამაზეც შემდგომ 
ვილაპარაკებ

10. write
პირველი სერვერიდან დაბრუნებული დასტურის შემდეგ ხდება მეორე სერვერისთვის write ის გამოძახება, server ის მხარეს
ვიყენებ pwrite მეთოდს, რომელსაც თავის მხრივ ფაილ დესკრიპტორი ჭირდება რომელიც პირველი სერვერის შემთხვევაში არის
fi->fh, ხოლო მეორე სერვერის შემთხვევაში არის კლიენტის მხარეს srfw_file სტრუქტურების მასივში მოძებნილი fd, ფაილის
სახელის მიხედვით 

11. release
ანალოგიურად პირველი სერვერის შემთხვევაში fi-fh ში შენახულ fd ს ვაძლევ სერვერს ხოლო, მეორე სერვერის შემთხვევაში
srfw_file სტრუქტურების მასივში ვეძებ სახელის შესაბამის fd ს

12. opendir
იძახება მხოლოდ პირველ სერვერზე და სერვერიდან მიბრუნდება DIR* რომელსაც (intptr_t) - დ  გადაკასტვის შემდეგ ვინახავ 
fi->fh ში

13. readdir
readdir იც მხოლოდ პირველ სერვერზე იძახება, სერვერი თავისთან გამოიძახებს readdir ს while ში მანამ სანამ null ს არ 
დაუბრუნებს დაბრუნებულ ფაილ / დირექტორიების სახელებს შეაწებებს ერთ ბაფერში delimiter ად მაქვს "/" რადგან თვითონ 
readdir სლეშებს არ აყოლებს path ს, ხოლო თვითონ ფაილის სახელში კი სლეში, რამდენიმე reserved char თან ერთად, არის ის 
char რომელიც არ შეგვხვდება, კლიენტი კი ამას strtok ფუნქციით დაჭრის და გადასცემს fuse filler ს.

14. releasedir
ხურავს პირველ სერვერზე გახსნილ დირექტორიას შესაბამისი DIR* ით რომელისაც კლიენტი იღებს fi->fh დან და კასტავს
(DIR*)(intptr_t).




---------------------------------------------------------
---------------------------------------------------------
RAID 1  |  STABLE STORAGE
---------------------------------------------------------
---------------------------------------------------------


>> Algorithms

ფაილის დასაჰეშად ვიყენებ MD5 ჰეშს, რომლის იმპლემენტაციასაც მივაგენი stackoverflow ზე 
(https://stackoverflow.com/questions/10324611/how-to-calculate-the-md5-hash-of-a-large-file-in-c)

სერვერის მხარეს დაჰეშვა და xattr ად შენახვა ხდება 1. mknod - ში ფაილის შექმნისას და 2. write ში 
რათა შეცვლილი ფაილისთვის მოხდეს ჰეშის დააფდეითება, ხოლო open ის დროს ხდება არსებული
ფაილის ხელახლა დაჰეშვა, ამ ფაილისთვის xattr ში არსებული ჰეშის ამოღება და ორივეს კლიენტისთვის
გამოგზავნა შესამოწმებლად, რადგან raid 1 ის შემთხვევაში გვაქვს mirroring ალგორითმი, კლიენტის მხარეს
ხელში გვიჭირავს 4 ჰეში პირველი სერვერის არსებული და ახლანდელი (h11, h12) და მეორე სერვერის არსებული და 
ახლანდელი (h21, h22), კლიენტი თავის მხრივ ამოწმებს შემდგომ ქეისებს

1. მომენტი, როცა პირველ სერვერზე დაზიანდა ფაილი და შედეგად h11 != h12 ს, ოღონდ h21 == h22 ს
ამ შემთხვევაში იძახება ressurect მეთოდი რომელიც აგზავნის მეორე სერვერზე read რექვესტს და თხოვს
ვაილში 4kb ზომის chunk ებს, რომლებსაც ამავე ვაილში უგზავნის პირველ სერვერს write რექვესტით

2. როდესაც პირიქით, მეორე სერვერზე დაზიანდა, აქაც ანალოგიურად ყველაფერი ოღნდ ხდება პირველი სერვერიდან
მეორე სერვერზე ინფორმაციის გადმოტანა

3. როცა h11 == h12 && h21 == h22, ეს ქეისი შეილება მოხდეს მაშნ, როცა ჩაწერის მომენტში, პირველ სერვერზე ჩაიწერა
დატა და დაგენერირდა ჰეში და მეორე სერვერზე ჩაიწერა ნაკლები დატა და დაგენერირდა შესაბამისი ჰეში, და სერვერ
ორს გონია რო ყველაფერი რიგზეა, ამ ქეისის შემოწმება შეგვილია h11  ს და h21 ის შედარებით, და თუ ისინი არ ემთხვევიან
ერთმანეთ მეორე სერვერზე არის დატა აღსადგენი და კლიენტი ანალოგიურად იძახებს ressurect მეთოდს 

4. როცა h11 != h12 && h21 != h22, ანუ ორივე სერვერზე დაზიანებულია ფაილი, ამ შემთხვევაში ჩავთვალე, რომ სწორი იქნებოდა
ორივე სერვერზე წამეშალა ეს დაზიანებული ფაილი



---------------------------------------------------------
---------------------------------------------------------
RAID 1  |  HOTSWAP
---------------------------------------------------------
---------------------------------------------------------

>> Data Structures 

//Data სტრუქტურა რომელიც გადაეცემა ტესტერ სრედებს, რომლებზეც ქვევით ვისაუბრებ
struct hotswap_data
{
	int sfd0; //sfd რომელზეც შესაბამისმა thread მა უნდა განახორციელოს წერა კითხვა
	int sfd1; //მეორე sfd, რომელსაც შემდგომ გამოიყენებს 
	pthread tid;  //thread ID
	pthread stid; //მეორე thread ის ID, რომელიც გამოიყენება შემდგომ გასათიშად
	int * status; //status პარამეტრი რომელიც ორივე thread მა უნდა იცოდეს, 
				  //თუ ჰოტსვაპი/სერვერის გათიშვა მოხდა რომ გააგებინოს ერთმა მეორეს
	struct NRF_Data * nrf_data; //fuse ის დატა log ირებისთვის საჭირო პარამეტრებისთვის და არამარტო
}

//დარჩენილი სერვერის მიერ დაბრუნებული სტრუქტურა რომელშიც წერია 
//1 ფაილის / დირექტორიის ინფო (არის დირექტორია თუ ფაილი) და path
struct hotswap_ret
{
	int regdir;   
	char path[256];
}


>> Algorithms

თავდაპირველად main ში და fork ვისას ანუ თით storagedisk ისთვის ისტარტება ორი სრედი შესაბამისი ინფორმაციით
შემდგომ კი ალგორითმი არის ასეთი, ეშვება ორი ტესტერი სრედი, რომლებიც პირველი პირველ სერვერზე და მეორე 
მეორე სერვერზე აგზავნის რექვესტებს და სერვერიდანაც ელოდება რისფონსს (რა თქმა უნდა ესეც და ყველა 
syscall დალოქილია nrd_data ში არსებულ საერთო ლოქზე), არის რაღაც res ცვლადი რომელსაც while ციკლში ეს 
ტესტერი სეტავს ყოველ ჯერზე როგორც 100 და თუ სერვერი გაითიშა ვერავინ ვერ გადააკეთებს და შესაბამისად
ამით ხდება იდენთიფიკაცია გაითიშა თუ არა, გათიშვის შემდეგ ვიმახსოვრებ პირველი გათიშვის მომენტს და შემდეგ ყოველ
ჯერზე ვამოწმებ რამდენი ხანია გასული difftime ფუნქციის საშუალებით, როგორც კი გავა timeout დრო, სერვერი ცხადდება 
დაკარგულად და იძახება hotswap ფუნქცია, რომელიც აკეთებს შემდეგს - სერვერს ეუბნება, რომ გამოძახება მოხდა hotswap
func num ით სერვერი კი იყენებს FTS ს ფუნქციონალს, რომ კლიენტს დაუბრუნოს ყველა დირექტორიის / საბდირექტორიის / 
ფაილის / საბ ფაილის path, ხოლო როგორც კი კლიენტი იღებს ამ ფაილის / დირექტორიის path ს იძახებს mknod / mkdir 
ებს hotswap სერვერზე, ასევე ფაილის შემთხვევაში იძახება ressurect ფუნქცია, რომელსაც ევალება ერთი სერვერიდან chunk
ებად ინფორმაციის წამოღება და მეორეში გადაწერა, საბოლოოდ კი hotswap ში გვექნება იგივე დატა რაც გადარჩენილ სერვერზე
იყო, ამის შემდეგ ხდება უკვე ჩანაცვლება დაკარგული სერვერის ჰოტსვაპით და status პარამეტრის 200 ად დასეტვა რაც იმას
ნიშნავს, რომ მოხდა hotswap და ამაზე აღარ შეამოწმოს.

აქ სრედები არ ამთავრებენ მუშაობას, მათ ასევე ევალებათ დარჩენილი 2 სერვერის გატესტვა, რასაც იგივე პრინციპით აკეთებენ
და თუ რომელიმემ დააფიქსირა, რომ მისი სატესტო სერვერი გაითიშა ელოდება timeout დრო და როდესაც გამოცხადდება დაკარგულად
ხდება დარჩენილი სერვერის აღქმა როგორც პირველი სერვერის და მეორე სერვერს sfd - დ ესეტება -1, რასაც შემდგომ syscall ებში
ვიყენებ იმის დასადგენად, უნდა გავუშვა თუ არა მეორე სერვერზე რექვესტი საერთოდ,  ხოლო thread ებში არსებული status ცვლადს
ესეტება 500, რაც იმის მანიშნებელია, რომ მათ თავის საქმე შეასრულეს და უნდა მორჩნენ მუშაობას.

hotswap ში იმპლემენტირებულია მხოლოდ, სერვერების ტესტირება, თუ დაიკარგა 1, hotswap ით ჩანაცვლება და hotswap სერვერზე დატას
გადაწერა, ამის შემდეგაც თუ კიდე დაიკარგა რომელიმე, უზრუნველყოფა იმის რომ დაკარგვის შემდგომ მხოლოდ პირველი სერვერი 
გაითვალისწინოს კონკრეტულმა syscall მა, არ არის იმპლემენტირებული ის ფაქტი, რომ თუ ამ timeout დროში რაიმე ცვლილება მოხდა
აღდგენის შემთხვევაში უნდა გაითვალისწინოს სერვერმა




---------------------------------------------------------
---------------------------------------------------------
CACHE
---------------------------------------------------------
---------------------------------------------------------

>> Data Structures

//ქეშის სტრუქტურა, რომელიც ინახავს მაქსიმალურ, შევსებულ ზომას, რაოდენობებსა და სტრუქტურების მასივს
//ფაილების კონტენტისა და მათ შესახებ ინფორმაციის შესანახად
struct Cache
{
	...
	struct cache_file ** files;
	struct cache_info ** infos;
};



//ფაილების(უფრო კონკრეტულად chunk ების) სტრუქტურა, ინახავს სახელს, ზომას, ოფსეტს, დატას და 
//last access time - ს eviction ისთვის, ჩათვლილია, რომ config file ში ყოველთვის LRU ალგორითმია მითითებული
struct cache_file
{
	char name[256];
	...
	time_t access;
	char * data;
};



//იგივე პრინციპით ინახავს ყველაფერს, ოღონდ data ს მაგივრად ინახავს პირდაპირ stat სტრუქტურას, რომელიც getattr
//syscall ისთვის გამოიყენება შემდგომ
struct cache_info
{
	char name[256];
	time_t access;
	struct stat * statbuf;
};


>> Algorithms

//File Cache

nrf_read syscall ში პირველ რიგში ვამოწმებ, მაქვს თუ არა შესაბამისი სახელით, size ითა და offset ით ქეშში entry
თუ მაქვს პირდაპირ იქ შენახულ დატას ვაბრუნებ buffer ში და ასევე ვაა update ბ  time_t access პარამეტრს, რომელიც შემდგომ
გამოიყენება LRU eviction ის დროს.
ხოლო თუ არ მაქვს ქეშში მაშინ : 


1. თუ საერთოდ არ ეტევა ქეშში რა თქმა უნდა არც ჩავარდება
2. თუ ეტევა მაგრამ არ არის ადგილი იწყება eviction while და ვარდება იმდენი entry რამდენიც საჭიროა ამ ფაილის (ჩანკის) ჩასაწერად

ანალოგიური იმპლემენტაციაა ფაილის ინფოს ქეშირებისთვის, რომელიც ხდება nrf_getattr syscall ში, eviction ისთვის 
ჩათვლილია, რომ ქეშის ზომაში ამ ინფოს ანუ struct stat ის ზომაც ითვლება და შედეგად შემოწმება ხდება არის თუ არა
საკმარისი sizeof(struct stat) ადგილი ქეშის მეხსიერებაში 

write ის დროს თავიდან მინდოდა დამეაფდეითებინა ქეში და ასეც მქონდა მაგრამ თუ ქეშს დავააფდეითებდი მაშინ სერვერზე არ უნდა გამეშვა და თუ სერვერზე გავუშვებდი უკვე ქეში აზრს კარგავდა ამ მომენტში, შესაბამისად ვამჯობინე write ის დროს ქეშიდან წაშლა 
რომელმაც ბევრად გაამარტივა write ის დროს წარმოქმნილი ქეისები და რეალურად ქეშიც მარტივი გახადა, ვფიქრობ ქეში მაქსიმალურად მარტივი და მოქნილი უნდა იყოს.


---------------------------------------------------------
---------------------------------------------------------
EPOLL API
---------------------------------------------------------
---------------------------------------------------------

>> Algorithms

ზოგადად epoll ის გამოყენებას აზრი აქვს, მაშინ როდესაც ერთი სერვერი რამდენიმე კლიენტს ემსახურება, ამ შემთხვევაში
epoll უყურებს fd ებს და ნახულობს ეხლა რომლის "მომსახურება" იქნება ყველაზე ოპტიმალური, ჩვენ შემთხვევაში 1-1 ზე
კავშირი გვაქვს სერვერ - კლიენტს შორის მაგრამ როგორც მითხრეს თქვენ გითქვამთ, რომ არ იქნებოდა პრობლემა, მთავარია
იმპლემენტაცია გვქონოდა.

ინიციალიზაციას ვუკეთებ epoll - ს epoll_create ით, ვეუბნები, რომ სულ 1 კლიენტი იქნება კავშირზე და ვაძლევ ამ კლიენტის
cfd ს, შემდეგ ჩემი request - response while ში ვიძახებ epoll_wait ს, რომელიც მიბრუნებს რამდენია მზად მომსახურებისთვის
და თანმიმდვრულად ვემსახურები მათ

---------------------------------------------------------
---------------------------------------------------------
LOGGING
---------------------------------------------------------
---------------------------------------------------------

>> Algorithms

ლოგირების ფაილის path გადმოეცემა კლიენტს კონფიგ ფაილიდან, რადგან ყველა დისკისთვის ერთია, fork ებამდე
იხსნება ეს ფაილი და ყველა fuse_main ს გადაეცემა NRF_Data ს მეშვეობით, ლოგირებას ურუნველყოფს ერთი ფუნქცია
log_msg, რომელსაც გადაეცემა, ფაილი, მესიჯი, დისკის სახელი და სერვერი, ეს ფუნქცია კი ითვლის ახლანდელ დროს და
ლოგ ფაილში მესიჯს წერს ამ სახით
[Sat Aug  4 19:50:51 2018] diskname 127.0.0.1:10001 open connection

პირველ რიგში ილოგება connection ები, როდის რომელი connection გაიხსნა თვითონ syscall ებში ილოგება cache დან
წაკთხვა / ჩამატება / eviction open syscall ში ილოგება stable storage ს ქეისები, როდის ერთი სერვერიდან მოხდა აღდგენა,
როდის მეორედან და როდის წაშლა ორივედა.

server testing ის დროს ილოგება, waiting... მაშინ როცა timeout დრო მიდის, ხოლო ამ დროის გასვლის შემდეგ ილოგება
თუ რომელი სერვერი გამოცხადდა დაკარგულად, ასევე ილოგება ის მომენტიც როცა ამ timeout ის დროს მოხდება სერვერის
აღდგენა.


---------------------------------------------------------
---------------------------------------------------------
შეჯამება
---------------------------------------------------------
---------------------------------------------------------

პროგრამის გაშვებისთვის საჭირო command / package / file ები აღწერილია /NET_RAID_FS/README.md ფაილში.

იმპლემენტირებულია:

1. config file ის პარსერი - არავალიდური config ფაილისას პროგრამა ითიშება და წერს შესაბამის ერორს
2. იერარქიუილი დირექტორიის სტრუქტურა - ამის არ დაწერა უფრო დიდი პრობლემა იქნებოდა 
3. Raid 1 mirroring - ორივე სერვერზე ხდება ინფორმაციის ჩაწერა, ასევე იმპლემენტირებულია stable storage
	მექანიზმი, რომელიც ადგენს ფაილის კონტენტის დაკარგვას და აღდგენს მას
4. Raid 5 არ არის იმპლემენტირებული, ამის სანაცვლოდ პროგრამა ეშვება ყოველთვის raid 1 ალგორითმით და
	სერვერებად იყენებს პირველ ორ სერვერს
5. მაღალმდგრადობა -  გათიშული სერვერი მომენტალურად არ ცხადდება დაკარგულად, ის ელოდება timeout
	დრო ამ timeout დროში წერა კითხვის ქეისი არც გათვალისწინებულია და სერვერთან კავშირის აღდგენის
	შემთხვევაში შესაბამისად არ ხდება აღდგენილ სერვერზე ინფორმაციის დაბრუნება

	ამ timeout დროის შემდეგ სერვერი ცხადდება დაკარგულად და ნაცვლდება hotswap სერვერით, რომელზეც 
	გადარჩენილი სერვერიდან ხდება დატას დუბლირება
6. ლოგირება - ილოგება მნიშვნელოვანი მომენტები კონფიგურაციის ფაილში მითითებულ errorlog ფაილში
7. პროექტის დოკუმენტაცია
8. ბონუსები :
	1. ქეშირება - იყენებს კონფიგურაციის ფაილში მითითებული ზომის მეხსიერებას და ქეშავს ფაილის
		კონტენტს და მათ შესახებ ინფორმაციას
	2. Epoll Api

პროგრამა იბილდება warning ების გარეშე, პროგრამა გატესტილია ვაგრანტში და მუშაობს იმპლემენტირებული ნაწილების
ყველა ქეისზე, fuse ის მეთოდებში გადატვირთულია ყველა საჭირო ფუნქცია + truncate. create ს მაგივრად გადატვირთულია 
mknod.

